{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXhR9wQ4lTwvZeJvLTooUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherlinvarshitha/Generative-AI/blob/main/2266_W4_ASS4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 1\n",
        "1.Design a simple ANN architecture with one input and one output layer (no hidden layer). Assume a linear activation function in the output layer. • Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with training data shown in Table 1. • Calculate the mean square error with training and testing data shown in Table 2. • Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the output with deployed ANN model\n"
      ],
      "metadata": {
        "id": "tTypAXvGIaOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R096HnUGD2SZ",
        "outputId": "c6fd1e9a-3224-432e-afb3-d0a6336f82d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 1.9026236782315746\n",
            "Epoch 100, MSE: 0.0013228288361012818\n",
            "Epoch 200, MSE: 0.0003458225189432219\n",
            "Epoch 300, MSE: 9.115950287081885e-05\n",
            "Epoch 400, MSE: 2.420315187263911e-05\n",
            "Epoch 500, MSE: 6.4657635407610204e-06\n",
            "Epoch 600, MSE: 1.736486161935597e-06\n",
            "Epoch 700, MSE: 4.686025033995313e-07\n",
            "Epoch 800, MSE: 1.271217041035109e-07\n",
            "Epoch 900, MSE: 3.479049086542068e-08\n",
            "Trained weights: [0.10067386 0.19984676 0.30026981]\n",
            "Trained bias: [-0.00027477]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "X_train = np.array([[0.1, 0.2, 0.3],\n",
        "                    [0.2, 0.3, 0.4],\n",
        "                    [0.3, 0.4, 0.5],\n",
        "                    [0.5, 0.6, 0.7],\n",
        "                    [0.1, 0.3, 0.5],\n",
        "                    [0.2, 0.4, 0.6],\n",
        "                    [0.3, 0.5, 0.7],\n",
        "                    [0.4, 0.6, 0.8],\n",
        "                    [0.5, 0.7, 0.1]])\n",
        "\n",
        "y_train = np.array([0.14, 0.20, 0.26, 0.38, 0.22, 0.28, 0.34, 0.40, 0.22])\n",
        "\n",
        "weights = np.random.rand(3)\n",
        "bias = np.random.rand(1)\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred = np.dot(X_train, weights) + bias\n",
        "    error = y_pred - y_train\n",
        "    mse = np.mean(error**2)\n",
        "\n",
        "    gradient_w = (2 / len(X_train)) * np.dot(X_train.T, error)\n",
        "    gradient_b = (2 / len(X_train)) * np.sum(error)\n",
        "    weights -= learning_rate * gradient_w\n",
        "    bias -= learning_rate * gradient_b\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse}\")\n",
        "\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained bias:\", bias)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x1, x2, x3):\n",
        "    input_data = np.array([x1, x2, x3])\n",
        "    prediction = np.dot(input_data, weights) + bias\n",
        "    return prediction\n",
        "x1 = float(input(\"Enter value for x1: \"))\n",
        "x2 = float(input(\"Enter value for x2: \"))\n",
        "x3 = float(input(\"Enter value for x3: \"))\n",
        "predicted_y = predict(x1, x2, x3)\n",
        "print(f\"The predicted output y is: {predicted_y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94OMwx55HBMw",
        "outputId": "635e7b46-7d9e-4553-a164-e8a4193bc358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter value for x1: 0.1\n",
            "Enter value for x2: 0.2\n",
            "Enter value for x3: 0.3\n",
            "The predicted output y is: [0.14665899]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 2\n",
        "Design a simple ANN architecture with one input and one output layer (no hidden layer). Assume a sigmoid activation function shown in the equation 1 in the output layer. f (x) = 1 1 + e−x (1) • Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with training data shown in Table"
      ],
      "metadata": {
        "id": "z69SluNHIwt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "X_train = np.array([[0.1, 0.2, 0.3],\n",
        "                    [0.2, 0.3, 0.4],\n",
        "                    [0.3, 0.4, 0.5],\n",
        "                    [0.5, 0.6, 0.7],\n",
        "                    [0.1, 0.3, 0.5],\n",
        "                    [0.2, 0.4, 0.6],\n",
        "                    [0.3, 0.5, 0.7],\n",
        "                    [0.4, 0.6, 0.8],\n",
        "                    [0.5, 0.7, 0.1]])\n",
        "y_train = np.array([0.5349, 0.5498, 0.5646, 0.5939, 0.5548, 0.5695, 0.5842, 0.5987, 0.5548])\n",
        "weights = np.random.rand(3)\n",
        "bias = np.random.rand(1)\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    z = np.dot(X_train, weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    error = y_pred - y_train\n",
        "    mse = np.mean(error**2)\n",
        "    d_loss_d_pred = 2 * error\n",
        "    d_pred_d_z = sigmoid_derivative(y_pred)\n",
        "    d_loss_d_z = d_loss_d_pred * d_pred_d_z\n",
        "    gradient_w = np.dot(X_train.T, d_loss_d_z) / len(X_train)\n",
        "    gradient_b = np.sum(d_loss_d_z) / len(X_train)\n",
        "    weights -= learning_rate * gradient_w\n",
        "    bias -= learning_rate * gradient_b\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse}\")\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained bias:\", bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOCZYsl_JgIQ",
        "outputId": "761b217a-d7ce-4dc9-dcf2-f936ef1c8321"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.062308076769100246\n",
            "Epoch 1000, MSE: 9.545012712670612e-05\n",
            "Epoch 2000, MSE: 4.155158293129368e-05\n",
            "Epoch 3000, MSE: 1.9267138889311853e-05\n",
            "Epoch 4000, MSE: 9.949586937144531e-06\n",
            "Epoch 5000, MSE: 5.996800510200843e-06\n",
            "Epoch 6000, MSE: 4.2817489295125286e-06\n",
            "Epoch 7000, MSE: 3.5074697766008538e-06\n",
            "Epoch 8000, MSE: 3.132002997587168e-06\n",
            "Epoch 9000, MSE: 2.9273221376629427e-06\n",
            "Trained weights: [-0.04823874  0.33709283  0.30003171]\n",
            "Trained bias: [-0.01821217]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([[0.6, 0.7, 0.8],\n",
        "                   [0.7, 0.8, 0.9]])\n",
        "y_test = np.array([0.6083, 0.6225])\n",
        "y_test_pred = sigmoid(np.dot(X_test, weights) + bias)\n",
        "mse_test = np.mean((y_test_pred - y_test) ** 2)\n",
        "print(f\"Test MSE: {mse_test}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zn2xEmjJ0gn",
        "outputId": "83bc67e0-b29f-43f0-e3d5-3fff4ea4f01e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 7.949786620648408e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x1, x2, x3):\n",
        "    input_data = np.array([x1, x2, x3])\n",
        "    z = np.dot(input_data, weights) + bias\n",
        "    prediction = sigmoid(z)\n",
        "    return prediction\n",
        "x1 = float(input(\"Enter value for x1: \"))\n",
        "x2 = float(input(\"Enter value for x2: \"))\n",
        "x3 = float(input(\"Enter value for x3: \"))\n",
        "predicted_y = predict(x1, x2, x3)\n",
        "print(f\"The predicted output y is: {predicted_y}\")\n"
      ],
      "metadata": {
        "id": "7lxpqSoHJ2Zf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}